{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netflix Movies and TV Show Analasis\n",
    "\n",
    "DataSet from: https://www.kaggle.com/shivamb/netflix-shows\n",
    "\n",
    "https://population.un.org/wpp/Download/Standard/CSV/\n",
    "\n",
    "https://data.worldbank.org/indicator/NY.GDP.MKTP.CD?view=map\n",
    "### Questions Being asked\n",
    "\n",
    "\n",
    "\n",
    "``` \n",
    "1: Where are most of Netflix titles being produced\n",
    "    a: When that country was added to the list of Netflix titles\n",
    "2: Network of the directors\n",
    "    \n",
    "```\n",
    "\n",
    "We will first import our data set using pandas and get it ready for processing\n",
    "\n",
    "lets first look at what kind of data attributes we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env BOKEH_RESOURCES=inline \n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from bokeh.io import output_notebook, show, output_file\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import GeoJSONDataSource, ColorBar, LogColorMapper\n",
    "from bokeh.palettes import brewer\n",
    "# output_notebook()\n",
    "\n",
    "#Displaying less rows as the previous size was pretty obnoxious\n",
    "pd.set_option('display.max_rows', 6)\n",
    "data = pd.read_csv (r'netflix_titles.csv')\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we want to try and figure out something about this data set\n",
    "\n",
    "The first thing that I want to look at is what country each tv show and movie is produced at.\n",
    "\n",
    "Lets first get a cleaner dataset only grabbing a list of country values\n",
    "\n",
    "Steps to take:\n",
    "    1. Obtain only the country column\n",
    "    2. Remove all values that are equal to N/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only grab the columns with title and country\n",
    "df = pd.DataFrame(data, columns= ['country'])\n",
    "# Drop all values that are NA\n",
    "df = df.dropna()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a cleaner dataset lets try and do some calculations on it, lets first sum up everytime we see a country to get an idea of the frequency a Netflix Show or Movie was made in one of these countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencyOfCountriesDict = df['country'].str.split(', |,', expand=True).stack().value_counts().to_dict()\n",
    "\n",
    "data = []\n",
    "for item in frequencyOfCountriesDict:\n",
    "    data.append([item, frequencyOfCountriesDict[item]])\n",
    "\n",
    "newdf = pd.DataFrame(data, columns=['CountryCode', 'frequency'])\n",
    "display(newdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the frequency of Netflix titles in each country lets now put this into a chart so it is easier to visualize the magnitude of the countires in relation to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(150,20))\n",
    "plt.bar(newdf['CountryCode'], newdf['frequency'], width=.5, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` For the above image double click on it to enlarge ```\n",
    "\n",
    "\n",
    "This bar chart doesnt give a good representation of the data in realation to other countries so lets try making a heat map of earth and see if we can gather some more conclusions based off of that\n",
    "\n",
    "We first start off with converting all the names of countries into their respective 3 letter country code and see which ones we know and the ones we dont know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "\n",
    "countries = {}\n",
    "for country in pycountry.countries:\n",
    "    countries[country.name] = country.alpha_3\n",
    "\n",
    "swapList = {}\n",
    "unknownCodes = {}\n",
    "for item in newdf.iterrows():\n",
    "    \n",
    "    if(countries.get(item[1]['CountryCode'], 'Unknown code') == 'Unknown code'):\n",
    "        unknownCodes[item[1]['CountryCode']] = countries.get(item[1]['CountryCode'], 'Unknown code')\n",
    "    else:\n",
    "        swapList[item[1]['CountryCode']] = countries.get(item[1]['CountryCode'], 'Unknown code')\n",
    "    \n",
    "# We see that the country name does not always map to a country code so we need to identify which ones we missed\n",
    "# This is in unknownCodes and then we need to define them as seen in the next block\n",
    "display(swapList, unknownCodes)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So some of the countries did not have a code attached to their name because the naming may have not matched up so lets set up a dictionary so we can explicitly map the corrrect countries to their respective code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknownCodeDict = {\n",
    "    'South Korea': 'KOR',\n",
    "    'Taiwan': 'TWN',\n",
    "    'Russia': 'RUS',\n",
    "    'Czech Republic': 'CZE',\n",
    "    'West Germany': 'DEU',\n",
    "    'Vietnam': 'VNM',\n",
    "    'Iran': 'IRN',\n",
    "    'Soviet Union': 'RUS',\n",
    "    'Venezuela': 'VEN',\n",
    "    'Vatican City': 'VAT',\n",
    "    'East Germany': 'DEU',\n",
    "    'Syria': 'SYR'\n",
    "}\n",
    "\n",
    "for country in unknownCodes:\n",
    "    if country in unknownCodeDict:\n",
    "        swapList[country] = unknownCodeDict[country]\n",
    "    \n",
    "display(swapList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a full dictionary of country codes\n",
    "\n",
    "We need to create a new dataframe (*not replace old one, I was told it was frowned upon to try and change the values like that*) with the new country codes that we aquired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFinal = []\n",
    "\n",
    "for index, item in newdf.iterrows():\n",
    "    if item[0] in swapList:\n",
    "        newCode = swapList[item[0]]\n",
    "        dataFinal.append([newCode, item[1]])\n",
    "\n",
    "\n",
    "dfFinal = pd.DataFrame(dataFinal, columns=['CountryCode', 'frequency'])\n",
    "display(dfFinal)\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we try and render our data onto the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFrequencyMap(data, nameOfColumnForCountryCode, frequencyColumnName, title=\"Default Title\"):\n",
    "    shapefile = './GeoShapeFile/ne_110m_admin_0_countries.shp'\n",
    "    #Read shapefile using Geopandas\n",
    "    gdf = gpd.read_file(shapefile)[['ADMIN', 'ADM0_A3', 'geometry']]\n",
    "    gdf.columns = ['country', 'country_code', 'geometry']\n",
    "\n",
    "    merged = gdf.merge(data, left_on='country_code', right_on=nameOfColumnForCountryCode)\n",
    "    merged_json = json.loads(merged.to_json())\n",
    "    json_data = json.dumps(merged_json)\n",
    "\n",
    "\n",
    "    #Input GeoJSON source that contains features for plotting.\n",
    "    geosource = GeoJSONDataSource(geojson = json_data)\n",
    "\n",
    "    #Define a sequential multi-hue color palette.\n",
    "    palette = brewer['Reds'][8]\n",
    "\n",
    "    #Reverse color order so that dark blue is highest obesity.\n",
    "    palette = palette[::-1]\n",
    "\n",
    "    #Instantiate LinearColorMapper that linearly maps numbers in a range, into a sequence of colors.\n",
    "    color_mapper = LogColorMapper(palette = palette, low = min(data[frequencyColumnName]), high = max(data[frequencyColumnName]))\n",
    "\n",
    "    #Create color bar. \n",
    "    color_bar = ColorBar(color_mapper=color_mapper, label_standoff=10,width = 10, height = 400,\n",
    "    border_line_color=None,location = (0,0), orientation = 'vertical' )\n",
    "\n",
    "    #Create figure object.\n",
    "    p = figure(title = title, plot_height = 500 , plot_width = 1000, toolbar_location = None)\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.ygrid.grid_line_color = None\n",
    "\n",
    "    #Add patch renderer to figure. \n",
    "    p.patches('xs','ys', source = geosource,fill_color = {'field' :frequencyColumnName, 'transform' : color_mapper},\n",
    "    line_color = 'black', line_width = 0.2, fill_alpha = 1)\n",
    "\n",
    "    #Specify figure layout.\n",
    "    p.add_layout(color_bar, 'right')\n",
    "\n",
    "    #Display figure inline in Jupyter Notebook.\n",
    "    output_notebook()\n",
    "\n",
    "    #Display figure.\n",
    "    show(p)\n",
    "    \n",
    "createFrequencyMap(dfFinal, 'CountryCode', 'frequency', 'Number of Netflix Titles Per Country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now by looking at the map and also the bar chart we can identify a few things\n",
    "\n",
    "1. America is the main location for producing movies (*which makes sense netflix is american based company*)\n",
    "So this by proximity would increase the number for mexico and canada.\n",
    "2. We also can see that India and China and india both have a good amount of titles but India far surpases \n",
    "China while having a smaller population, maybe the reasoning for why that is something we can explore\n",
    "3. I want to say that the frequency correlated to gdp of the country, population and maybe something with most spoken languages\n",
    "4. This does not take time into account so we could see when netflix added the movie to their library to see if they initially started with a core audience and then branched out\n",
    "\n",
    "Lets first look at the 2nd question and maybe just do a little bit of research and see if we can figure anything out\n",
    "\n",
    "quickly found https://www.techinasia.com/netflix-china-nope which is good to know that its an already researched topic and the awnser was that there are more restrictions in china in order to get their content out there so they take on less chineese titles on their platform because they would not be able to reach that user base. We can now take into account that this maybe the same effect for other countries. We may say that if I were netflix then I would want to invest said country produced title I would want to take into account the population inside the nation and the global population of people from that nation. We can now try and formulate a method to see how many titles produced in certain countries that we invest in. \n",
    "\n",
    "Lets take a look at another heat map of every contries gdp and see if it lines up with the one above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryGDPData = pd.read_csv (r'country_gdp.csv')\n",
    "countryGDPDataDf = pd.DataFrame(countryGDPData, columns= ['Country Code', '2018'])\n",
    "countryGDPDataDf = countryGDPDataDf.dropna()\n",
    "#display(countryGDPDataDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createFrequencyMap(countryGDPDataDf, 'Country Code', '2018', 'Map of every Countries gdp in 2018')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "populationData = pd.read_csv (r'world_population.csv')\n",
    "\n",
    "# Process data\n",
    "# Meaning get sum of all age groups in a country by year and also change the country code\n",
    "populationFrequencyDf = pd.DataFrame(populationData, columns= ['Location', 'Time', 'PopTotal'])\n",
    "populationFrequencyDf = populationFrequencyDf[populationFrequencyDf.Time == 2019]\n",
    "populationFrequencyDf = populationFrequencyDf.drop(columns='Time')\n",
    "\n",
    "FinalPopFreqDf = pd.DataFrame(columns=['Location', 'PopTotal'])\n",
    "for idx, row in populationFrequencyDf.iterrows():\n",
    "    if row.Location in FinalPopFreqDf:\n",
    "#         FinalPopFreqDf['Location'][row.Location] += row.PopTotal\n",
    "        #FinalPopFreqDf = FinalPopFreqDf[FinalPopFreqDf == row.Location ]\n",
    "       print(\"hello\")\n",
    "    \n",
    "    else:\n",
    "        FinalPopFreqDf.append({'Location': row.Location, 'PopTotal': row.PopTotal}, ignore_index=True)\n",
    "       \n",
    "print(FinalPopFreqDf)\n",
    "#         FinalPopFreqDf['Location'][row.Location] = row.PopTotal\n",
    "# display(FinalPopFreqDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
